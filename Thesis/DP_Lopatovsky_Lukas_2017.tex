% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=M,english]{FITthesis}[2012/10/20]

% \usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

\usepackage{graphicx} %graphics files inclusion
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[chapter]{algorithm} % http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode} % http://ctan.org/pkg/algorithmicx
\usepackage{pseudocode}
\usepackage{comment}
\usepackage{enumitem}

\usepackage{dirtree} %directory tree visualisation

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries

%%%%%%%%%%%% own commands %%%%%%%%%%%%%%%%%%
\newcommand{\matr}[1]{\mathbf{#1}} 
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Department of Theoretical Computer Science }
\title{Thesis title (SPECIFY)}
\authorGN{Luk{\' a}{\v s}} %author's given name/names
\authorFN{Lopatovsk{\' y}} %author's surname
\author{Luk{\' a}{\v s} Lopatovsk{\' y}} %author's name without academic degrees
\authorWithDegrees{Bc. Luk{\' a}{\v s} Lopatovsk{\' y}} %author's name with academic degrees
\supervisor{Ing. Daniel Va{v s}ata, Ph.D.}
\acknowledgements{THANKS (remove entirely in case you do not with to thank anyone)}
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Replace with comma-separated list of keywords in Czech.}
\keywordsEN{HMM, CT-HMM.}
\declarationOfAuthenticityOption{1} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\setsecnumdepth{part}

\begin{introduction}
	This is the master thesis. welcome!
	\section{Motivation and objectives}
	About a discrete model [inspired by Rabiner? ] and why it is not satisfactory  %(as http://delivery.acm.org/10.1145/350000/343402/p162-aziz.pdf?ip=147.32.98.33&id=343402&acc=ACTIVE%20SERVICE&key=D6C3EEB3AD96C931%2E9BD1EC80ACA8C1C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=702042173&CFTOKEN=62280276&__acm__=1481276923_1a71eb623e1c42d4c8e9b84178eb248f)	
\end{introduction}

\setsecnumdepth{all}

\chapter{Discrete-time Hidden Markov Model}
This chapter will briefly discuss Discrete-time Hidden Markov Model (later just HMM) as defined in [Rabiner], where you can also look for the more comprehensive explanation with examples.

We have decided to write this chapter, because Continuous-time HMM is its direct logical extension and also it is using some of the subroutines that are the same as if defined for discrete time model.

\section{Discrete Markov Process}
    
Discrete Markov Process is the stochastic process in discrete equidistant time $t= 1,2,\cdots$, that in every time is in some state from the set of $N$ distinct states $S_1,S_2, \cdots, S_N $. The change of the state can occur in every $t$, with some given probability $a_{ij}$, what is the probability that the system will move from the state $S_i$ to the state $S_j$ in any time step, or possibly it can remain in the same state with probability $a_{ii}$. Then we denote the actual state at time $t$ as $q_t$.

To tell about the system to be Markov, it should hold so called Markov property. It can be expressed by equation \eqref{eq:mp}, it means that the probability of transition to another state depends only on the current state and doesn't use the information about any past states - the system is memory-less.

\begin{equation}\label{eq:mp}
\begin{aligned}
\mathrm{P}(q_t & = S_j \mid q_{t-1} = S_{j-1}, q_{t-2} = S_{j-2}, \cdots)  \\   
               & = \mathrm{P}(q_t = S_j \mid q_{t-1} = S_{j-1} )
\end{aligned}
\end{equation}

The other important property is Homogeneity. The probability of transition from one state to another is constant throughout the time. 

\begin{equation}\label{eq:homo}
   a_{ij} = \mathrm{P}(q_t = S_j \mid q_{t-1} = S_{j-1} ),\qquad 1 \leq i,j \leq N, \forall t
\end{equation}

For the transition probabilities are applied classical stochastic constrains.

\begin{equation}
   a_{ij} \geq 0
\end{equation}

\begin{equation}
   \sum_{j=1}^N a_{ij} = 1
\end{equation}

\section{Discrete-time Hidden Markov Model}

Discrete-time Hidden Markov Model is the extension of the previous model. In the classic Markov Model we can directly see the states we are currently in as observations. Such a model is of restricted use and doesn't fit well to many problems. In HMM are the states observable by another probabilistic function that for each of the states assign the probabilities that the observation $O = O_1,O_2,\cdots,O_T$ will occur. It forms the doubly embedded stochastic process, where the underlying one is hidden and observable only by the statistical properties of another process dependable upon it.

An HMM can be described by its:

\begin{enumerate}[resume]
\item \textbf{Hidden States}
\begin{equation}
S = \{ S_1,S_2, \cdots, S_N \}
\end{equation} 
\item \textbf{State Transition Probability Distribution}
\begin{equation}
A = \{ a_{ij} \}, \quad 1 \leq i,j \leq N
\end{equation} 
\item \textbf{Observation Symbols}
\begin{equation}
V = \{ V_1,V_2, \cdots, V_M \} 
\end{equation}
\item \textbf{Observation Symbols Probability Distribution} \\
$B = \{ b_{i}(k) \}$, where $b_{i}(k)$ is the probability that the observation $k$ will occur, if the system is currently in state $i$. 
\begin{equation}
b_i(k) = \mathrm{P}(V_k \text{ at t } \mid q_t = S_i), \qquad 1 \leq i \leq N, \quad 1 \leq k \leq M
\end{equation}
\item \textbf{Initial state distribution} \\
$\pi = \{ \pi_i \}$, where $\pi_i$ is the probability of the initial state being $S_i$.
\begin{equation}
\pi_{i} = \mathrm{P}(q_1 = S_i), \qquad 1 \leq i \leq N
\end{equation}
\end{enumerate}

For the convenience we will declare parameter $\lambda = \{A,B,\pi\}$, compactly denoting the set of all parameters of the model.   

\section{Three Basic Problems for HMMs}
% citate: Jack Ferguson of IDA in lectures at Bell laboratories
When dealing with real-world application we need to deal with following problems, as proposed in [citace]. First we will describe the problems and later in the successive subsections we will explain the algorithms that can effectively solve them. 

\begin{enumerate}
\item Compute the probability $ \mathrm{P}(O|\lambda) $ of the observation sequence $O = O_1,O_2,\cdots,O_T$, given the set of parameters $\lambda = \{A,B,\pi\}$. 
\item Choose the optimal state sequence $Q = q_1,q_2,\cdots,q_T$, having the observation sequence $O$ and parameters $\lambda$.
\item Adjust the model parameters $\lambda$ in the way it maximizes the probability of observation sequence $ \mathrm{P}(O|\lambda) $. 
\end{enumerate}


\subsection{Forward-Backward Algorithm}
The Forward-Backward Algorithm is actually the pair of separate algorithms (Forward vs. Backward). We will explain the Forward one and at the end we will describe the modifications that are needed to do for the other. To solve the first proposed problem we could get by each of them separately, however we need to define both of them for the use in the algorithms later in the text.  

Forward Algorithm is the Dynamic Programming algorithm taking advantage from the Markov property - the independence upon past events. We will denote the forward variable as $\alpha_t(i)$ defined as
\begin{equation}
\alpha_t(i) = \mathrm{P}(O_1,O_2,\cdots,O_t,q_t = S_i \mid \lambda )
\end{equation}

that can be gradually counted by following equations for $t=1$ and $t=t+1$.

\begin{equation}
\alpha_1(i) = \pi_i b_i(O_1), \qquad 1 \leq i \leq N
\end{equation}

\begin{equation}
\begin{aligned}
\alpha_{t+1}(i) = \left( \sum_{j=1}^N \alpha_t(j) a_{ji} \right) b_i(O_{t+1}), \qquad 1& \leq t \leq T - 1, \\                                                                                  1& \leq j \leq N																			
\end{aligned}
\end{equation}
  
Now we can obtain the solution of the first problem simply by summing through the all forward variables in the time $T$.

\begin{equation}
\mathrm{P}(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
\end{equation} 

Similarly we can define backward variable $\beta_t(i)$ as
\begin{equation}
\beta_t(i) = \mathrm{P}(O_{t+1},O_{t+2},\cdots,O_T,q_t = S_i \mid \lambda ) 
\end{equation}
The DP algorithm can be derived analogically as the one for the Forward.

We need to count $N$ variables at each time-step, each of that takes exactly $N$ operations to evaluate. It makes overall complexity $\mathcal{O}(N^2T)$.    

\subsection{Individually Most Likely States Sequence}
There is more ways how we can look at the world "optimal" in the problem 2 statement. One of the possible approaches is to maximize the expected number of correctly assigned states. To solve it, we need to define the variable determining the probability of being in specific state in particular time.

\begin{equation}
\gamma_t(i) = \mathrm{P}(q_t = S_i \mid O,\lambda ) 
\end{equation}

Here we can use already defined forward and backward variables and count the $\gamma_t(i)$ as
\begin{equation}
\begin{aligned}
\gamma_t(i) &= \frac{ \mathrm{P}( q_t = S_i, O \mid \lambda )}{ \mathrm{P}( O \mid \lambda )} =
               \frac{  \alpha_t(i) \beta_t(i) }{ \mathrm{P}( O \mid \lambda )} = \\
            &= \frac{  \alpha_t(i) \beta_t(i) }{ \sum_j=1^N \alpha_t(j) \beta_t(j) } 
\end{aligned}
\end{equation}

To get the desired individual most likely state $q_t$, it is enough to find one of the highest probability.

\begin{equation}
q_t = \argmax_{1 \leq i \leq N} \gamma_t(i), \qquad 1 \leq t \leq T
\end{equation} 

Applying this algorithm to the whole sequence will lead to the highest expected number of correctly assigned states, however such a sequence as a whole can have low probability or in extreme cases can not even be feasible. This would happen if probability of transition among two consecutive states in the sequence was zero.   


   
\subsection{Viterbi Algorithm} there is more ways what can be considered optimal in this case ... more:[citate]
\subsection{Baum-Welch Algorithm}

\chapter{Finite State Continuous Time Markov Chain} 

%releationship with discrete (construction)? sum(n = 0..inf) p(n(t)) u(i,j)^n 
%construction from discrete HMM is probably important, heterogenous MM.
\section{Definition}
%cit:[S.Karlin]
A finite state continuous time Markov chain (later as CTMC) is a~stochastic process $X_t (t > 0)$ on the states $i_0, i_1, \cdots, i_n, i, j \in C$ (for $n>0$ and $C$ being the finite state space), that in any time $0 \leq s_0 < s_1 < \cdots < s_n < s$ satisfies the following equations: 
\begin{itemize}
\item \textbf{Markov property:} Probability of transition from state $i$ to state $j$ during the time interval $t$ is stationary i.e. independent on the states of process in the times $< s$. The process is memoryless.    
    
\begin{equation}
\begin{aligned}
& \mathrm{P}( X_{t+s} = j \mid X_s = i, X_{s_n} = i_n, \cdots , X_{s_0} ) = \\ 
& = \mathrm{P}( X_{t+s} = j \mid X_s = i )
\end{aligned}
\end{equation}

The equation describes that the stochastic process is memory-less.

\item \textbf{Homogeneity:} Probability of transition in any given time $s \geq 0$ depends only at the length of the time interval $t \geq 0$. 

\begin{equation}
\begin{aligned}
& \mathrm{P}( X_{t+s} = j \mid X_s = i ) = \mathrm{P}( X_t = j \mid X_0 = i ) = \\
& = \mathrm{p_t}(i,j)
\end{aligned}
\end{equation}

\end{itemize}

\section{Properties} 

Holding the upper mentioned conditions the transition rate $\mathrm{p_t}(i,j), i,j \in C$ satisfies:%cit:[S.Karlin]


\begin{equation}
\mathrm{p_t}(i,j) \geq 0
\end{equation}

\begin{equation}
\sum_{j \in C} \mathrm{p_t}(i,j) = 1
\end{equation}

\begin{equation}\label{eq:l0}  
\lim_{t \to 0^+} \mathrm{p_t}(i,j)= 
\begin{cases}
1, i = j\\
0, i \neq j
\end{cases}
\end{equation}

\begin{equation}\label{eq:ckeq}
\sum_{k \in C}\mathrm{p_s}(i,k)\mathrm{p_t}(k,j) = \mathrm{p_{s+t}}(i,j)     
\end{equation}

Equation \eqref{eq:ckeq} is known as Chapman-Kolmogorov equation. 
We can define the matrix $\mathrm{P_t}$, where the entry $(i,j)$ is $\mathrm{P_t(i,j)}$ to get the equation in following form:

\begin{equation}
\matr{P_s} \matr{P_t} = \matr{P_{s+t}},   \qquad t,s > 0  
\end{equation}

?proof of Chapman-kolmogorov?

%construction from Discrete time with poisson timing

\subsection{ Jump Rates }

Using the Chapman-Kolmogorov equation \eqref{eq:ckeq}, if we know the $\mathrm{p_t(i,j)}$ for every $i,j$ and $0 < t < t_0$, we can compute the values for any time $t > 0$. The property \eqref{eq:l0} asserts that $\mathrm{P_t(i,j)}$ is continuous for $t=0$,so for $t>0$ ref[S.Karlin] and we can find it's right derivative in 0. This knowledge enables us to determine the $\mathrm{p_t(i,j)}$ for any given time $t>0$.   

\begin{equation}
\mathrm{q}(i,j) =  \left.\frac{\mathrm{d}\mathrm{p_t}(i,j)}{\mathrm{dt}} \,\right|_{t=0} = \lim_{t \to 0^+} \frac{\mathrm{p_t}(i,j)}{t}, \quad \text{if } i\neq j     
\end{equation}

We will call this derivative $\mathrm{q}(i,j)$ the jump rate from state i to some other state j. The jump rate $\mathrm{q}(i,i)$ can be derived from equation of transition probabilities summing to the one.

\begin{equation}
1 = \mathrm{p_t}(i,i) + \sum_{j \in C ,j \neq i} \mathrm{p_t}(i,j) 
\end{equation}

dividing by time interval $t$ and letting $t$ decrease close to zero, we will obtain following equation. 

\begin{equation}\label{eq:qii}
 \mathrm{q}(i,i) =  \sum_{j \in C ,j \neq i} \mathrm{q}(i,j) 
\end{equation}

(We assume the finite rates $\mathrm{q}(i,j)$, infinite rate would immediately leads to the leaving of the state, so it makes no sense to consider.)\\

It can be shown that by construction of the CTMC from discrete time Markov chain (DTMC) and  it's underlying Poisson process with rate $\lambda$, the equation $\mathrm{q}(i,j) = \lambda\mathrm{u}(i,j)$ holds. Where $\mathrm{u}(i,j)$ is the transition rate from the $i$ to $j$ in DTMC. This is why we call the $\mathrm{q}(i,j)$ the jump rate.[zdroj]  

%ROUTING MATRIX - u(i, j) = q(i, j) / lambda max
%u(i, i) = 1 - sum_j u(i, j)

\subsection{The Infinitesimal Matrix}

We can also look at $\mathrm{q}(i,i)$ as at the rate in which the $X_t$ is leaving the state $i$ \eqref{eq:ckeq}. Then we can define the Infinitesimal matrix $\matr{Q}$ as following:

\begin{equation}  
\matr{Q}(i,j)= 
\begin{cases}
\mathrm{q}(i,j), \text{if } i\neq j\\
- \sum_{k \in C ,k \neq i} \mathrm{q}(i,k), \quad \text{if } i=j
\end{cases}
\end{equation}

Such a defined matrix will hold $\pi\matr{Q} = 0$... [explain more, proof, $e^{\matr{Q}t}$]


\section{Continuous Time Hidden Markov Model}


\chapter{Expectation-Maximization Algorithm}


%(1) Maximum Likelihood from Incomplete Data via the EM Algorithm , A. P. Dempster; N. M. Laird; D. B. Rubin, Journal of the Royal Statistical Society. Series B (Methodological), Vol. 39, No. 1. (1977), pp.  1-38.

Expectation-Maximization (EM) Algorithm (first introduces in [1]) is the method for finding the Maximum likelihood estimates(MLE) of parameters in probabilistic models over the incomplete data-set ( i.e. data-set containing unobserved (latent) variables ). It is a natural generalization of maximum likeli-
hood estimation, however the latent variables makes finding of the MLE more difficult. To count it directly can be very computationally expensive, so the EM algorithm use the iterative approach to approximate the solution by repeating the sequence of simpler consecutive steps.

Let's have vector $\mathbf{x} = (x_{1},x_{2},\dotsc,x_{n})$ containing the known (observed) variables and vector $\mathbf{z} = (z_{1},z_{2},\dotsc,z_{n})$ containing latent (unobserved) variables. We will set $\theta$ for the unknown parameters we want to estimate. We will use the notation $\mathrm{logP}(\mathbf{x},\mathbf{z};\theta)$ as the log-likelihood function, which estimates the probabilities for the given parameters. Notice that we are using the log-likelihood to avoid underflow of parameters values that could otherwise easily arise during the computation.  
%[st]simle tutorial: What is the expectation maximization algorithm? Chuong B Do & Serafim Batzoglou

Look at [st] for more detailed, easily understandable tutorial with examples.    

\begin{enumerate}
\item \textbf{Initialization:} We will make the initial guess of the parameters $\hat \theta^{(0)} $.
\end{enumerate} 

We will continue by iterating through two following steps: 

\begin{enumerate}[resume]
\item \textbf{Expectation:} Calculate the expected value of the likelihood function $\mathrm{logP}(\mathbf{x},\mathbf{z};\hat \theta^{(t)})$. Where $\hat \theta^{(0)}$ is the current parameters estimation. We will find the function $\mathrm{Q}$ that lower bounds $\mathrm{logP}(\mathbf{x},\mathbf{z};\hat \theta^{(t)})$.

\begin{equation}\label{eq:exp}
 \mathrm{Q}(\hat\theta^{(t)}) =  \mathrm{logP}(\mathbf{x},\mathbf{z};\hat \theta^{(t)})
\end{equation}

\item \textbf{Maximization:} Find the value $\hat\theta^{(t+1)}$ that maximize $\mathrm{Q}(\hat\theta^{(t)})$.  
\end{enumerate}  

As the value of $\mathrm{Q}(\hat\theta^{(t)})$ matches the log-likelihood function at $\hat\theta^{(t)}$, it follows that $ \mathrm{Q}(\hat\theta^{(t)}) =  \mathrm{logP}(\mathbf{x},\mathbf{z};\hat \theta^{(t)}) \leq \mathrm{Q}(\hat\theta^{(t+1)}) =  \mathrm{logP}(\mathbf{x},\mathbf{z};\hat \theta^{(t+1)}) $ thus the function value form the non-decreasing order. [nejaky dokaz?]. We will stop the computation ones the parameter converges to some value, that is not changing.  

It is important to notice that this approach leads to the local optimum. To get better results the algorithm can be launched more times starting always with different random initialization. So much some other optimization technique can be used [sources?].    



    



\chapter{Baum-Welch}

%continuous time Baum-Welch -> citace veci, ktore to robili - DP, spaniely, starsi clanok. + preco je novsi clanok najlepsi.

%\chapter{Realisation}

\setsecnumdepth{part}
\chapter{Conclusion}


\bibliographystyle{iso690}
\bibliography{mybibliographyfile}

\setsecnumdepth{all}
\appendix

\chapter{Acronyms}
% \printglossaries
\begin{description}
	\item[DTMC] Discrete-time Markov Chain	
	\item[CTMC] Continuous-time Markov Chain
	\item[DT-HMM] Discrete-time Hidden Markov Model
	\item[CT-HMM] Continuous-time Hidden Markov Model
	\item[EM] Expectation-Maximization 
	\item[MLE] Maximum likelihood estimation
\end{description}


\chapter{Contents of enclosed CD}

%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
