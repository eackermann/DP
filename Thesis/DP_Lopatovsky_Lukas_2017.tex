% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=M,english]{FITthesis}[2012/10/20]

% \usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

\usepackage{graphicx} %graphics files inclusion
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[chapter]{algorithm} % http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode} % http://ctan.org/pkg/algorithmicx
\usepackage{pseudocode}
\usepackage{comment}

\usepackage{dirtree} %directory tree visualisation

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries

%%%%%%%%%%%% own commands %%%%%%%%%%%%%%%%%%
\newcommand{\matr}[1]{\mathbf{#1}} 

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Department of Theoretical Computer Science }
\title{Thesis title (SPECIFY)}
\authorGN{Luk{\' a}{\v s}} %author's given name/names
\authorFN{Lopatovsk{\' y}} %author's surname
\author{Luk{\' a}{\v s} Lopatovsk{\' y}} %author's name without academic degrees
\authorWithDegrees{Bc. Luk{\' a}{\v s} Lopatovsk{\' y}} %author's name with academic degrees
\supervisor{Ing. Daniel Va{v s}ata, Ph.D.}
\acknowledgements{THANKS (remove entirely in case you do not with to thank anyone)}
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Replace with comma-separated list of keywords in Czech.}
\keywordsEN{Fabulous, Great, Extraordinary.}
\declarationOfAuthenticityOption{1} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\setsecnumdepth{part}

\begin{introduction}
	This is the master thesis. welcome!
	\section{Motivation and objectives}
	About a discrete model and why it is not satisfactory  %(as http://delivery.acm.org/10.1145/350000/343402/p162-aziz.pdf?ip=147.32.98.33&id=343402&acc=ACTIVE%20SERVICE&key=D6C3EEB3AD96C931%2E9BD1EC80ACA8C1C5%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=702042173&CFTOKEN=62280276&__acm__=1481276923_1a71eb623e1c42d4c8e9b84178eb248f)	
	
	
\end{introduction}

\setsecnumdepth{all}

\chapter{Discrete-time Markov Chain}
- under consideration
\chapter{Discrete-time Hidden Markov Model}
- under consideration


\chapter{Finite State Continuous Time Markov Chain} 

%releationship with discrete (construction)? sum(n = 0..inf) p(n(t)) u(i,j)^n 
%construction from discrete HMM is probably important, heterogenous MM.
\section{Definition}
%cit:[S.Karlin]
A finite state continuous time Markov chain (later as CTMC) is a~stochastic process $X_t (t > 0)$ on the states $i_0, i_1, \cdots, i_n, i, j \in C$ (for $n>0$ and $C$ being the finite state space), that in any time $0 \leq s_0 < s_1 < \cdots < s_n < s$ satisfies the following equations: 
\begin{itemize}
\item \textbf{Markov property:} Probability of transition from state $i$ to state $j$ during the time interval $t$ is stationary i.e. independent on the states of process in the times $< s$. The process is memoryless.    
    
\begin{equation}
\begin{aligned}
& \mathrm{P}( X_{t+s} = j \mid X_s = i, X_{s_n} = i_n, \cdots , X_{s_0} ) = \\ 
& = \mathrm{P}( X_{t+s} = j \mid X_s = i )
\end{aligned}
\end{equation}

The equation describes that the stochastic process is memoryless.

\item \textbf{Homogeneity:} Probability of transition in any given time $s \geq 0$ depends only at the length of the time interval $t \geq 0$. 

\begin{equation}
\begin{aligned}
& \mathrm{P}( X_{t+s} = j \mid X_s = i ) = \mathrm{P}( X_t = j \mid X_0 = i ) = \\
& = \mathrm{p_t}(i,j)
\end{aligned}
\end{equation}

\end{itemize}

\section{Properties} 

Holding the upper mentioned conditions the transition rate $\mathrm{p_t}(i,j), i,j \in C$ satisfies:%cit:[S.Karlin]


\begin{equation}
\mathrm{p_t}(i,j) \geq 0
\end{equation}

\begin{equation}
\sum_{j \in C} \mathrm{p_t}(i,j) = 1
\end{equation}

\begin{equation}\label{eq:l0}  
\lim_{t \to 0^+} \mathrm{p_t}(i,j)= 
\begin{cases}
1, i = j\\
0, i \neq j
\end{cases}
\end{equation}

\begin{equation}\label{eq:ckeq}
\sum_{k \in C}\mathrm{p_s}(i,k)\mathrm{p_t}(k,j) = \mathrm{p_{s+t}}(i,j)     
\end{equation}

Equation \eqref{eq:ckeq} is known as Chapman-Kolmogorov equation. 
We can define the matrix $\mathrm{P_t}$, where the entry $(i,j)$ is $\mathrm{P_t(i,j)}$ to get the equation in following form:

\begin{equation}
\matr{P_s} \matr{P_t} = \matr{P_{s+t}},   t,s > 0  
\end{equation}

?proof of Chapman-kolmogorov?

%construction from Discrete time with poisson timing

\subsection{ Jump Rates }

Using the Chapman-Kolmogorov equation \eqref{eq:ckeq}, if we know the $\mathrm{p_t(i,j)}$ for every $i,j$ and $0 < t < t_0$, we can compute the values for any time $t > 0$. The property \eqref{eq:l0} asserts that $\mathrm{P_t(i,j)}$ is continuous for $t=0$,so for $t>0$ ref[S.Karlin] and we can find it's right derivative in 0. This knowledge enables us to determine the $\mathrm{p_t(i,j)}$ for any given time $t>0$.   

\begin{equation}
\mathrm{q}(i,j) =  \left.\frac{\mathrm{d}\mathrm{p_t}(i,j)}{\mathrm{dt}} \,\right|_{t=0} = \lim_{t \to 0^+} \frac{\mathrm{p_t}(i,j)}{t}, \text{if } i\neq j     
\end{equation}

We will call this derivative $\mathrm{q}(i,j)$ the jump rate from state i to some other state j. The jump rate $\mathrm{q}(i,i)$ can be derived from equation of transition probabilities summing to the one.

\begin{equation}
1 = \mathrm{p_t}(i,i) + \sum_{j \in C ,j \neq i} \mathrm{p_t}(i,j) 
\end{equation}

dividing by time interval $t$ and letting $t$ decrease close to zero, we will obtain following equation. 

\begin{equation}\label{eq:qii}
 \mathrm{q}(i,i) =  \sum_{j \in C ,j \neq i} \mathrm{q}(i,j) 
\end{equation}

(We assume the finite rates $\mathrm{q}(i,j)$, infinite rate would immediately leads to the leaving of the state, so it makes no sense to consider.)\\

It can be shown that by construction of the CTMC from discrete time Markov chain (DTMC) and  it's underlying Poisson process with rate $\lambda$, the equation $\mathrm{q}(i,j) = \lambda\mathrm{u}(i,j)$ holds. Where $\mathrm{u}(i,j)$ is the transition rate from the $i$ to $j$ in DTMC. This is why we call the $\mathrm{q}(i,j)$ the jump rate.[zdroj]  

%ROUTING MATRIX - u(i, j) = q(i, j) / lambda max
%u(i, i) = 1 - sum_j u(i, j)

\subsection{The Infinitesimal Matrix}

We can also look at $\mathrm{q}(i,i)$ as at the rate in which the $X_t$ is leaving the state $i$ \eqref{eq:ckeq}. Then we can define the Infinitesimal matrix $\matr{Q}$ as following:

\begin{equation}  
\matr{Q}(i,j)= 
\begin{cases}
\mathrm{q}(i,j), \text{if } i\neq j\\
- \sum_{k \in C ,k \neq i} \mathrm{q}(i,k), \text{if } i=j
\end{cases}
\end{equation}

Such a defined matrix will hold $\pi\matr{Q} = 0$... [explain more, proof, $e^{\matr{Q}t}$]


\section{Continuous Time Hidden Markov Model}


\chapter{Expectation-Maximization Algorithm}


%(1) Maximum Likelihood from Incomplete Data via the EM Algorithm , A. P. Dempster; N. M. Laird; D. B. Rubin, Journal of the Royal Statistical Society. Series B (Methodological), Vol. 39, No. 1. (1977), pp.  1-38.

Expectation-Maximization (EM) Algorithm (first introduces in [1]) is the method for finding the Maximum likelihood estimation (MLE) in probabilistic models over the incomplete data-set ( i.e. data-set containing unobserved (latent) variables ). As to find the MLE directly can be very computationally expensive, the EM algorithm use the iterative approach to approximate the solution by repeating the sequence of simpler consecutive steps.  

\begin{enumerate}
\item \textbf{Initialization:} $\hat \theta^{(0)} $
\item \textbf{Expectation:} 
\item \textbf{Maximization:}
\end{enumerate}  

...when finito?, It is important to notice that this approach leads to the local optimum. To get better results the algorithm can be launched more times starting always with different random initialization. So much some other optimization technique can be used [sources?].    

%simle tutorial: What is the expectation maximization algorithm? Chuong B Do & Serafim Batzoglou

    



\chapter{Baum-Welch}

%continuous time Baum-Welch -> citace veci, ktore to robili - DP, spaniely, starsi clanok. + preco je novsi clanok najlepsi.

\chapter{Realisation}

\setsecnumdepth{part}
\chapter{Conclusion}


\bibliographystyle{iso690}
\bibliography{mybibliographyfile}

\setsecnumdepth{all}
\appendix

\chapter{Acronyms}
% \printglossaries
\begin{description}
	\item[DTMC] Discrete-time Markov Chain	
	\item[CTMC] Continuous-time Markov Chain
	\item[DT-HMM] Discrete-time Hidden Markov Model
	\item[CT-HMM] Continuous-time Hidden Markov Model
	\item[EM] Expectation-Maximization 
	\item[MLE] Maximum likelihood estimation
\end{description}


\chapter{Contents of enclosed CD}

%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
